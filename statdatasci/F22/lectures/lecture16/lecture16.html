<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Bernoulli and Binomial Distribution</title>
    <meta charset="utf-8" />
    <meta name="author" content="Xiao Hui Tai" />
    <meta name="date" content="2022-10-31" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/default-fonts.css" rel="stylesheet" />
    <link href="libs/font-awesome/css/all.css" rel="stylesheet" />
    <link href="libs/font-awesome/css/v4-shims.css" rel="stylesheet" />
    <link href="libs/panelset/panelset.css" rel="stylesheet" />
    <script src="libs/panelset/panelset.js"></script>
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Bernoulli and Binomial Distribution
]
.subtitle[
## <br><br> STA35A: Statistical Data Science 1
]
.author[
### Xiao Hui Tai
]
.date[
### October 31, 2022
]

---






layout: true
  
&lt;!-- &lt;div class="my-footer"&gt; --&gt;
&lt;!-- &lt;span&gt; --&gt;
&lt;!-- &lt;a href="https://datasciencebox.org" target="_blank"&gt;datasciencebox.org&lt;/a&gt; --&gt;
&lt;!-- &lt;/span&gt; --&gt;
&lt;!-- &lt;/div&gt;  --&gt;

---

&lt;style type="text/css"&gt;
.tiny .remark-code { font-size: 60%; }
.small .remark-code { font-size: 80%; }
.tiny { font-size: 60%; }
.small { font-size: 80%; }
&lt;/style&gt;





## Recap
--

- Random variables

  - Expectation and variance
  
  - Discrete and continuous random variables

- Common probability distributions

  - Bernoulli 


---
## Today
- Common probability distributions

  - Bernoulli 
  
  - Binomial 

- Law of large numbers 

---
## Recall: Bernoulli random variable

- `\(Y=1\)` is often called a "success", `\(Y=0\)` is called a "failure", and `\(\pi\)` or `\(p\)` is defined as the probability of a success. 

- The probability of a "failure" is then `\(1 - p\)`

- Examples:

  - Coin flip: let `\(Y=1\)` if heads and `\(Y=0\)` if tails, then `\(P(Y=1) = p=0.5\)`

  - Vegetarian in US: `\(Y=1\)` if vegetarian and `\(Y=0\)` if not, then `\(P(Y=1) = p=0.05\)` and `\(P(Y=0)=1-p=1-0.05=0.95\)`

  - Vegetarian in India: `\(Y=1\)` if vegetarian and `\(Y=0\)` if not, then `\(P(Y=1)=p=0.31\)` and `\(P(Y=0)=1-p=1-0.31=0.69\)`

---
## Sampling from a Bernoulli distribution in R

- When `\(p = .5\)`, we can use the `sample()` function


```r
set.seed(0) # so results are reproducible 
bernoulliDraws &lt;- sample(0:1, size = 100, replace = TRUE)
bernoulliDraws
```

```
##   [1] 1 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0
##  [31] 1 1 1 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 0 1 1 0 0 1 1 1
##  [61] 0 0 1 1 1 1 1 1 0 1 1 1 1 0 0 0 1 1 0 0 1 1 1 0 0 0 1 0 1 0
##  [91] 1 0 0 1 1 0 0 0 1 1
```

---
## Sampling from a Bernoulli distribution in R


```r
data.frame(outcome = bernoulliDraws) %&gt;%
  ggplot(aes(x = outcome)) +
    geom_bar()
```

&lt;img src="lecture16_files/figure-html/unnamed-chunk-4-1.png" width="60%" style="display: block; margin: auto;" /&gt;

---
## Sampling from a Bernoulli distribution in R

What happens when we increase the sample size? 

.small[
.pull-left[
`\(n = 100\)`

```r
set.seed(0) # so results are reproducible 
bernoulliDraws &lt;- sample(0:1, size = 100, 
                         replace = TRUE)
data.frame(outcome = bernoulliDraws) %&gt;%
  ggplot(aes(x = outcome)) +
    geom_bar()
```

&lt;img src="lecture16_files/figure-html/unnamed-chunk-5-1.png" width="100%" style="display: block; margin: auto;" /&gt;
]


.pull-right[
`\(n = 1000\)`

```r
set.seed(0) # so results are reproducible 
bernoulliDraws &lt;- sample(0:1, size = 1000, 
                         replace = TRUE)
data.frame(outcome = bernoulliDraws) %&gt;%
  ggplot(aes(x = outcome)) +
    geom_bar()
```

&lt;img src="lecture16_files/figure-html/unnamed-chunk-6-1.png" width="100%" style="display: block; margin: auto;" /&gt;
]
]

---
## Sampling from a Bernoulli distribution in R

What happens when we increase the sample size? 

.small[
.pull-left[
`\(n = 1000\)`

```r
set.seed(0) # so results are reproducible 
bernoulliDraws &lt;- sample(0:1, size = 1000, 
                         replace = TRUE)
data.frame(outcome = bernoulliDraws) %&gt;%
  ggplot(aes(x = outcome)) +
    geom_bar()
```

&lt;img src="lecture16_files/figure-html/unnamed-chunk-7-1.png" width="100%" style="display: block; margin: auto;" /&gt;
]

.pull-right[
`\(n = 10000\)`

```r
set.seed(0) # so results are reproducible 
bernoulliDraws &lt;- sample(0:1, size = 10000, 
                         replace = TRUE)
data.frame(outcome = bernoulliDraws) %&gt;%
  ggplot(aes(x = outcome)) +
    geom_bar()
```

&lt;img src="lecture16_files/figure-html/unnamed-chunk-8-1.png" width="100%" style="display: block; margin: auto;" /&gt;
]
]

---
## Law of large numbers

The law of large numbers states that as a sample size grows, the sample mean gets closer to the expected value, or population mean

.small[

```r
myMeans &lt;- data.frame(sampleSize = 1:10000, myMean = NA)
meanFun &lt;- function(inputSampSize, outcomes) {
  return(mean(outcomes[1:inputSampSize]))
}
myMeans$myMean &lt;- sapply(myMeans$sampleSize, meanFun, bernoulliDraws)
head(myMeans)
```

```
##   sampleSize    myMean
## 1          1 1.0000000
## 2          2 0.5000000
## 3          3 0.6666667
## 4          4 0.5000000
## 5          5 0.4000000
## 6          6 0.5000000
```

```r
tail(myMeans)
```

```
##       sampleSize    myMean
## 9995        9995 0.5011506
## 9996        9996 0.5011004
## 9997        9997 0.5010503
## 9998        9998 0.5010002
## 9999        9999 0.5010501
## 10000      10000 0.5010000
```
]
---
## Law of large numbers

```r
myMeans %&gt;%
  ggplot(aes(x = sampleSize, y = myMean)) +
  geom_line() +
  labs(x = "Sample size",
       y = "Sample mean",
       title = "Illustration of law of large numbers")
```

&lt;img src="lecture16_files/figure-html/unnamed-chunk-10-1.png" width="60%" style="display: block; margin: auto;" /&gt;


---
## Sampling from a Bernoulli distribution in R

- For any value of `\(0 \leq p \leq 1\)` (including .5), we can use the `rbinom()` function

- `rbinom()` has the arguments `n, size, prob`, where `n` is the number of draws from the distribution, and `prob` is the probability of success.

- `size` is the "number of trials (zero or more)" -- for the Bernoulli distribution, we use `size = 1`. Let's leave this for now and come back to it after we've discussed the binomial distribution.


```r
set.seed(0) # so results are reproducible 
inputP &lt;- .3
bernoulliDraws.3 &lt;- rbinom(n = 100, size = 1, prob = inputP)
bernoulliDraws.3
```

```
##   [1] 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0 0 1
##  [31] 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0
##  [61] 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 1 1 0 1 0 0 1 0 1 0 0
##  [91] 0 0 0 0 1 1 1 0 0 1
```

---
## Sampling from a Bernoulli distribution in R


```r
mean(bernoulliDraws.3)
```

```
## [1] 0.33
```

```r
data.frame(outcome = bernoulliDraws.3) %&gt;%
  ggplot(aes(x = outcome)) +
    geom_bar()
```

&lt;img src="lecture16_files/figure-html/unnamed-chunk-12-1.png" width="60%" style="display: block; margin: auto;" /&gt;

---
## Effect of changing parameter p

.small[
.pull-left[

```r
set.seed(0) # so results are reproducible 
inputP &lt;- .3
bernoulliDraws.3 &lt;- rbinom(n = 100, size = 1, prob = inputP)
data.frame(outcome = bernoulliDraws.3) %&gt;%
  ggplot(aes(x = outcome)) +
    geom_bar() +
    labs(title = "100 Bernoulli draws, p = .3")
```

&lt;img src="lecture16_files/figure-html/unnamed-chunk-13-1.png" width="100%" style="display: block; margin: auto;" /&gt;
]

.pull-right[

```r
set.seed(0) # so results are reproducible 
inputP &lt;- .7
bernoulliDraws.7 &lt;- rbinom(n = 100, size = 1, prob = inputP)
data.frame(outcome = bernoulliDraws.7) %&gt;%
  ggplot(aes(x = outcome)) +
    geom_bar() +
    labs(title = "100 Bernoulli draws, p = .7")
```

&lt;img src="lecture16_files/figure-html/unnamed-chunk-14-1.png" width="100%" style="display: block; margin: auto;" /&gt;
]
]
---
## Recall: Law of large numbers

The law of large numbers states that as the sample size grows, the sample mean gets closer to the expected value, or population mean

- Sample size is the number of draws from the distribution 

- Sample mean is the mean among those samples 




&lt;img src="lecture16_files/figure-html/unnamed-chunk-16-1.png" width="60%" style="display: block; margin: auto;" /&gt;

---
.small[

```r
set.seed(0) # so results are reproducible 
inputP &lt;- .3
bernoulliDraws &lt;- rbinom(n = 5000, size = 1, prob = inputP)
myMeans &lt;- data.frame(sampleSize = 1:5000, myMean = NA)
meanFun &lt;- function(inputSampSize, outcomes) {
  return(mean(outcomes[1:inputSampSize]))
}
myMeans$myMean &lt;- sapply(myMeans$sampleSize, meanFun, bernoulliDraws)
```


```r
head(myMeans)
```

```
##   sampleSize    myMean
## 1          1 1.0000000
## 2          2 0.5000000
## 3          3 0.3333333
## 4          4 0.2500000
## 5          5 0.4000000
## 6          6 0.3333333
```

```r
tail(myMeans)
```

```
##      sampleSize    myMean
## 4995       4995 0.3053053
## 4996       4996 0.3052442
## 4997       4997 0.3051831
## 4998       4998 0.3051220
## 4999       4999 0.3050610
## 5000       5000 0.3050000
```
]

---

```r
myMeans %&gt;%
  ggplot(aes(x = sampleSize, y = myMean)) +
  geom_line() +
  labs(x = "Sample size",
       y = "Sample mean",
       title = "Illustration of law of large numbers with p = .3")
```

&lt;img src="lecture16_files/figure-html/unnamed-chunk-19-1.png" width="60%" style="display: block; margin: auto;" /&gt;


---
## From Bernoulli to binomial... 

- `\(Y\)` takes value 1 with probability `\(p\)` and value 0 with probability `\(1-p\)`

- `\(P(Y=y)=p^y(1-p)^{1-y}\)`

  - `\(P(Y=1)=p^1(1-p)^0=p\)` (remember `\(x^0=1\)` for any `\(x\)`)
  
  - `\(P(Y=0)=p^0(1-p)^1=1-p\)`
  
- For the Bernoulli random variable, we don't really need this formality

- However, we want to extend this to more complex settings

- For example, in a randomly-selected group of 3 high school students, how surprising would it be to get 2 who have smoked e-cigarettes in the past month?

- Could consider three draws from a Bernoulli distribution 

---

## Case Study: E-Cigarettes

- The [CDC reports](https://www.cdc.gov/tobacco/data_statistics/fact_sheets/youth_data/tobacco_use/index.htm) that 19.6% of high school students have smoked e-cigarettes in the past 30 days. We'll round this to 20% for simplicity.

- `\(P(Y=1)=P(Smoker)=p=0.2\)` and `\(P(Y=0)=0.8\)`

- Now suppose we randomly select two independent high school students and define a new random variable `\(X\)` representing the number of smokers. X can take the values 0, 1, or 2. 

- Let `\(Y_1\)` be the smoking status of the first student and `\(Y_2\)` be the smoking status of the second student, where `\(Y_j = 1\)` if student `\(j\)` smokes and 0 otherwise.

.pull-left[
Next we'll talk about how to get the *probability distribution* of `\(X\)`.

]
.pull-right[
| `\(Y_1\)` | `\(Y_2\)` | `\(X\)` | `\(P(X)\)` |
|:----:|:----:|:----:|:----:|
| 0 | 0 | 0 | |
| 1 | 0 | 1 | |
| 0 | 1 | 1 | |
| 1 | 1 | 2 | |
]

---
## Case Study: E-Cigarettes

- Recall: If events A and B are independent, then `\(P(A \cap B)=P(A)\times P(B).\)`

- Let `\(A_1\)` be the event that `\(Y_1=1\)` and let `\(A_2\)` be the event that `\(Y_2=1\)`. 

- Since the students are independent, 

$$
`\begin{aligned}
P(Y_1=Y_2=1) &amp; = P(A_1 \cap A_2) \\
 &amp; = P(A_1) P(A_2) \\
 &amp; = p \times p  \\
 &amp;= 0.2(0.2)\\
 &amp;=0.04. 
\end{aligned}`
$$

---
## Case Study: E-Cigarettes

.pull-left[
Now we can fill in the bottom row of the probability distribution of `\(X\)`.

]
.pull-right[
| `\(Y_1\)` | `\(Y_2\)` | `\(X\)` | `\(P(X)\)` |
|:----:|:----:|:----:|:----:|
| 0 | 0 | 0 | |
| 1 | 0 | 1 | |
| 0 | 1 | 1 | |
| 1 | 1 | 2 |  `\(0.02 \times 0.02 =0.04\)` |
]

---
## Case Study: E-Cigarettes

.pull-left[
It's straightforward to fill in the rest of the table in the same way
]
.pull-right[
| `\(Y_1\)` | `\(Y_2\)` | `\(X\)` | `\(P(X)\)` |
|:----:|:----:|:----:|:----:|
| 0 | 0 | 0 | `\(0.8 \times 0.8 = 0.64\)` |
| 1 | 0 | 1 | `\(0.2 \times 0.8 = 0.16\)` |
| 0 | 1 | 1 | `\(0.8 \times 0.2 = 0.16\)` |
| 1 | 1 | 2 | `\(0.2 \times 0.2 =0.04\)` |
]

---

## Case Study: E-Cigarettes

.pull-left[

Recall our table:

| `\(Y_1\)` | `\(Y_2\)` | `\(X\)` | `\(P(X)\)` |
|:----:|:----:|:----:|:----:|
| 0 | 0 | 0 | `\(0.8 \times 0.8 = 0.64\)` |
| 1 | 0 | 1 | `\(0.2 \times 0.8 = 0.16\)` |
| 0 | 1 | 1 | `\(0.8 \times 0.2 = 0.16\)` |
| 1 | 1 | 2 | `\(0.2 \times 0.2 =0.04\)` |
]
.pull-right[
We can clean up the table to get the probability distribution of `\(X\)`:

| | | | |
|:--:|:--:|:--:|:--:|
| `\(X\)` | 0 | 1 | 2 |
| `\(P(X=x)\)` | 0.64 | 0.32 | 0.04 |

So if we randomly sample two US high schoolers, the probability that both are recent e-cig smokers is 0.04 (4% chance), the probability only one recently smoked is 0.32 (this can happen two ways -- either only the first smoked or only the second smoked), and the probability neither smoked e-cigs recently is 0.64.
]

---
## Case Study: E-Cigarettes

Now suppose we randomly sample 3 independent high school students

| `\(Y_1\)` | `\(Y_2\)` | `\(Y_3\)` | `\(X\)` | `\(P(X)\)` |
|:----:|:----:|:----:|:----:|:----:|
| 0 | 0 | 0 | 0 |  |
| 1 | 0 | 0 | 1 |  |
| 0 | 1 | 0 | 1 |  |
| 0 | 0 | 1 | 1 |  |
| 1 | 1 | 0 | 2 |  |
| 1 | 0 | 1 | 2 |  |
| 0 | 1 | 1 | 2 |  |
| 1 | 1 | 1 | 3 |  |

---

## Case Study: E-Cigarettes

Because these are independent high school students, we can calculate the probabilities in the same manner as before.


| `\(Y_1\)` | `\(Y_2\)` | `\(Y_3\)` | `\(X\)` | `\(P(X)\)` |
|:----:|:----:|:----:|:----:|:----:|
| 0 | 0 | 0 | 0 | 0.8(0.8)(0.8)=0.512 |
| 1 | 0 | 0 | 1 | 0.2(0.8)(0.8)=0.128 |
| 0 | 1 | 0 | 1 | 0.8(0.2)(0.8)=0.128 |
| 0 | 0 | 1 | 1 | 0.8(0.8)(0.2)=0.128 |
| 1 | 1 | 0 | 2 | 0.2(0.2)(0.8)=0.032 |
| 1 | 0 | 1 | 2 | 0.2(0.8)(0.2)=0.032 |
| 0 | 1 | 1 | 2 | 0.8(0.2)(0.2)=0.032 |
| 1 | 1 | 1 | 3 | 0.2(0.2)(0.2)=0.008 |

The probability that 2 of 3 are recent e-cig smokers is `\(0.032+0.032+0.032=0.096\)` or 9.6%

---

## Case Study: E-Cigarettes

.pull-left[
| `\(Y_1\)` | `\(Y_2\)` | `\(Y_3\)` | `\(X\)` | `\(P(X)\)` |
|:----:|:----:|:----:|:----:|:----:|
| 0 | 0 | 0 | 0 | 0.8(0.8)(0.8)=0.512 |
| 1 | 0 | 0 | 1 | 0.2(0.8)(0.8)=0.128 |
| 0 | 1 | 0 | 1 | 0.8(0.2)(0.8)=0.128 |
| 0 | 0 | 1 | 1 | 0.8(0.8)(0.2)=0.128 |
| 1 | 1 | 0 | 2 | 0.2(0.2)(0.8)=0.032 |
| 1 | 0 | 1 | 2 | 0.2(0.8)(0.2)=0.032 |
| 0 | 1 | 1 | 2 | 0.8(0.2)(0.2)=0.032 |
| 1 | 1 | 1 | 3 | 0.2(0.2)(0.2)=0.008 |
]

.pull-right[
The probability distribution of `\(X\)`, the number of recent e-cig smokers out of three high school students, is now 
 
| | | | | |
|:--:|:--:|:--:|:--:|:---:|
| `\(X\)` | 0 | 1 | 2 | 3|
| `\(P(X=x)\)` | 0.512 | 0.384 | 0.096 | 0.008 |
 
 ]

---

## Case Study: E-Cigarettes

- Extending to 4 and more students, we can see why computing the probablities by hand, as we've done, is intractable

- We can use the **binomial distribution** to describe this random variable 

---

## Binomial random variable

- The **binomial distribution** gives us the probability of `\(X\)` "successes" from a sequence of `\(n\)` independent Bernoulli trials (size `\(n\)`). This is often denoted binomial(n, p).

- In our example, each student would represent an independent Bernoulli trial (either an e-cig smoker, or not).

  - 1 draw from the binomial distribution is made of 3 independent draws from the Bernoulli distribution 

- This distribution involves three assumptions.

  - There is a fixed number `\(n\)` of Bernoulli trials, each of which results in one of two mutually-exclusive outcomes
  
  - The outcomes of the `\(n\)` trials are independent
  
  - The probability of success `\(p\)` is the same for each trial

---

## Binomial distribution

- The probability mass function for the binomial distribution is given by `\(P(X=x)=\begin{pmatrix} n \\ x \end{pmatrix}p^x(1-p)^{n-x}\)` 

- Compare this with `\(P(Y=y)=p^y(1-p)^{1-y}\)` for the Bernoulli distribution.

- First, look at the second part, `\(p^x(1-p)^{n-x}\)`. This is just multiplying the right combination of `\(p\)` and `\(1-p\)` as in the previous tables.

  - There will be a total of `\(n\)` terms being multiplied, one probability for each draw of the distribution (each student in this case)

  - For example, if we want the probability of 3 e-cig smokers, `\(X=3\)`, the second part is `\(p^x(1-p)^{n-x}=0.2^3(0.8)^0=0.008\)`, just as in the table.

---

## Binomial distribution

`$$P(X=x)=\begin{pmatrix} n \\ x \end{pmatrix}p^x(1-p)^{n-x}$$`

- If we want the probability of 2 e-cig smokers and 1 non-smoker, i.e., `\(x=2\)`, the second part is `\(p^x(1-p)^{n-x}=0.2^2(0.8)^{3-2}=0.032\)`, which is what we see in any single row in which we have two smokers and one non-smoker.

  - This is the probability of any one specific combination of 2 smokers and 1 nonsmoker. Then we need to figure out how many combinations of 2 smokers and 1 nonsmoker we could get.

- The first part, `\(\begin{pmatrix}n \\x \end{pmatrix}\)`, accounts for all the possible ways in which we can have 2 smokers out of 3 people.

---

## Combinations

- The first part, `\(\begin{pmatrix} n \\ x \end{pmatrix}\)`, is pronounced "n choose x" 

- This is called a **combination**.

- In this particular setting, it is also called the *binomial coefficient*. 

- It is a formula for the number of ways to pick `\(x\)` subjects from a larger group of `\(n\)` and is defined as `$$\begin{pmatrix} n \\ x \end{pmatrix} = \frac{n!}{x!(n-x)!}.$$`

- n! (pronounced "n factorial") is shorthand for the recursive multiplication `\(n!=n(n-1)(n-2)\cdots (1)\)`. 

  - `\(3!=3(2)(1)=6\)`
  - `\(4!=4(3)(2)(1)=24\)`, and so forth. 
  - We define `\(0!=1\)`.

---

## Combinations
- In R, combinations are done using `choose(n, k)`

- Factorials are `factorial(x)`, e.g., 

```r
factorial(3)
```

```
## [1] 6
```

- How many ways can we pick 3 subjects from a group of 3? Using the binomial coefficient, we see it is `\(\begin{pmatrix}3 \\ 3 \end{pmatrix}=\frac{3!}{3!0!}=\frac{3(2)(1)}{3(2)(1)1}=1\)`. In R,

```r
choose(3, 3)
```

```
## [1] 1
```

---

## Combinations

Going back to the table, we see there are 3 ways to pick 2 subjects from a group of 3 students.  

.small[

| `\(Y_1\)` | `\(Y_2\)` | `\(Y_3\)` | `\(X\)` | `\(P(X)\)` |
|:----:|:----:|:----:|:----:|:----:|
| 0 | 0 | 0 | 0 | 0.8(0.8)(0.8)=0.512 |
| 1 | 0 | 0 | 1 | 0.2(0.8)(0.8)=0.128 |
| 0 | 1 | 0 | 1 | 0.8(0.2)(0.8)=0.128 |
| 0 | 0 | 1 | 1 | 0.8(0.8)(0.2)=0.128 |
| 1 | 1 | 0 | 2 | 0.2(0.2)(0.8)=0.032 |
| 1 | 0 | 1 | 2 | 0.2(0.8)(0.2)=0.032 |
| 0 | 1 | 1 | 2 | 0.8(0.2)(0.2)=0.032 |
| 1 | 1 | 1 | 3 | 0.2(0.2)(0.2)=0.008 |

]

We can also calculate `\(\begin{pmatrix}3 \\ 2 \end{pmatrix}=\frac{3!}{2!1!}=\frac{3(2)(1)}{(2)(1)(1)}=3\)`.

---

## Binomial probabilities

Putting the ingredients together, we have 

$$
`\begin{aligned}
P(X = 2) &amp;=  \begin{pmatrix} n \\ x \end{pmatrix}p^x(1-p)^{n-x}\\
&amp;= \begin{pmatrix} 3 \\ 2 \end{pmatrix}p^2(1-p)^{3-2}\\
&amp;=3(0.2)^2(0.8)^{3-2}\\
&amp;=3(.032) \\
&amp;=.096
\end{aligned}`
$$
This is the same as we saw in the table. 

---

## Binomial probabilities 

- We saw how to calculate `\(P(X = 2)\)`. How about `\(P(X = 3)\)`?

$$
`\begin{aligned}
P(X = x) &amp;=\begin{pmatrix} n \\ x \end{pmatrix} p^x(1-p)^{n-x} \\
&amp;= \frac{n!}{x!(n-x)!}p^x(1-p)^{n-x} 
\end{aligned}`
$$

- Using the binomial distribution, the probability of getting 3 recent e-cig smokers in our sample of 3 students is

`$$\begin{aligned}
P(X = 3)&amp; = \begin{pmatrix} 3 \\ 3 \end{pmatrix}0.2^3(1-0.2)^{3-3}  \\
&amp; = \frac{3!}{3!(3-3)!}0.2^3(1-0.2)^{3-3} \\
 &amp; = \frac{3(2)(1)}{3(2)(1)1}0.2^3(0.8)^{0}=0.2^3 \\
 &amp; = 0.008.
 \end{aligned}$$`

- In this way, we can derive the probability distribution

---
## Binomial probabilites in R

- Luckily, we don't have to do this by hand

- In R, we can use `dbinom()`

- From the help page: `dbinom()` has the arguments `x, size, prob, log = FALSE`
  - `x` are the values of `\(x\)` that we want probabilities for; here it is 0, 1, 2 and 3
  - `size` is the number of Bernoulli trials; here it is 3
  - `prob` is the probability of success; here it is .2

.pull-left[

```r
dbinom(x = 0:3, size = 3, 
       prob = .2)
```

```
## [1] 0.512 0.384 0.096 0.008
```
]

.pull-right[
The probability distribution of `\(X\)`, the number of recent e-cig smokers out of three high school students, is now 
 
| | | | | |
|:--:|:--:|:--:|:--:|:---:|
| `\(X\)` | 0 | 1 | 2 | 3|
| `\(P(X=x)\)` | 0.512 | 0.384 | 0.096 | 0.008 |
 
 ]


---
## Bernoulli vs. binomial random variable

- Recall: The **binomial distribution** gives us the probability of `\(X\)` "successes" from a sequence of `\(n\)` independent Bernoulli trials. 

- In our example, each student would be an independent Bernoulli trial (either an e-cig smoker, or not).

  - Bernoulli random variable: whether or not a student is a recent e-cigarette smoker 

  - Binomial random variable: number of students, out of 3, who are recent e-cigarette smokers

  - One observation of this binomial random variable is composed of 3 Bernoulli trials ("size 3")
  
- The Bernoulli distribution can be thought of as a special case of the binomial distribution, with 1 "trial"

---
## Mean and variance of binomial random variable

For a binomial random variable with number of trials `\(n\)` and probability of success `\(p\)`, `\(E(X) = np\)` and `\(Var(X) = n(p)(1-p)\)`

- In our example, `\(p = .2\)`, `\(n = 3\)`, so `\(E(X) = .6\)` and `\(Var(X) = 3*.2*.8 = .48\)`

--

- The Bernoulli distribution is a special case of the binomial distribution, with `\(n = 1\)`. Recall: `\(E(X) = p\)` and `\(Var(X) = p(1-p)\)`.



---
## Summary
- Common probability distributions

  - Bernoulli 
  
  - Binomial 



    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
