---
title: "Bernoulli and Binomial Distribution"
subtitle: "<br><br> STA35A: Statistical Data Science 1"
author: "Xiao Hui Tai"
date: October 31, 2022
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      # ratio: "16:9"
      # highlightLines: true
      # highlightStyle: solarized-light
      countIncrementalSlides: false
---


```{r child = "../setup.Rmd"}
```

```{css, echo = FALSE}
.tiny .remark-code { font-size: 60%; }
.small .remark-code { font-size: 80%; }
.tiny { font-size: 60%; }
.small { font-size: 80%; }
```


```{r packages, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
```


## Recap
--

- Random variables

  - Expectation and variance
  
  - Discrete and continuous random variables

- Common probability distributions

  - Bernoulli 


---
## Today
- Common probability distributions

  - Bernoulli 
  
  - Binomial 

- Law of large numbers 

---
## Recall: Bernoulli random variable

- $Y=1$ is often called a "success", $Y=0$ is called a "failure", and $\pi$ or $p$ is defined as the probability of a success. 

- The probability of a "failure" is then $1 - p$

- Examples:

  - Coin flip: let $Y=1$ if heads and $Y=0$ if tails, then $P(Y=1) = p=0.5$

  - Vegetarian in US: $Y=1$ if vegetarian and $Y=0$ if not, then $P(Y=1) = p=0.05$ and $P(Y=0)=1-p=1-0.05=0.95$

  - Vegetarian in India: $Y=1$ if vegetarian and $Y=0$ if not, then $P(Y=1)=p=0.31$ and $P(Y=0)=1-p=1-0.31=0.69$

---
## Sampling from a Bernoulli distribution in R

- When $p = .5$, we can use the `sample()` function

```{r}
set.seed(0) # so results are reproducible 
bernoulliDraws <- sample(0:1, size = 100, replace = TRUE)
bernoulliDraws
```

---
## Sampling from a Bernoulli distribution in R

```{r}
data.frame(outcome = bernoulliDraws) %>%
  ggplot(aes(x = outcome)) +
    geom_bar()
```

---
## Sampling from a Bernoulli distribution in R

What happens when we increase the sample size? 

.small[
.pull-left[
$n = 100$
```{r out.width = "100%"}
set.seed(0) # so results are reproducible 
bernoulliDraws <- sample(0:1, size = 100, 
                         replace = TRUE)
data.frame(outcome = bernoulliDraws) %>%
  ggplot(aes(x = outcome)) +
    geom_bar()
```
]


.pull-right[
$n = 1000$
```{r out.width = "100%"}
set.seed(0) # so results are reproducible 
bernoulliDraws <- sample(0:1, size = 1000, 
                         replace = TRUE)
data.frame(outcome = bernoulliDraws) %>%
  ggplot(aes(x = outcome)) +
    geom_bar()
```
]
]

---
## Sampling from a Bernoulli distribution in R

What happens when we increase the sample size? 

.small[
.pull-left[
$n = 1000$
```{r out.width = "100%"}
set.seed(0) # so results are reproducible 
bernoulliDraws <- sample(0:1, size = 1000, 
                         replace = TRUE)
data.frame(outcome = bernoulliDraws) %>%
  ggplot(aes(x = outcome)) +
    geom_bar()
```
]

.pull-right[
$n = 10000$
```{r out.width = "100%"}
set.seed(0) # so results are reproducible 
bernoulliDraws <- sample(0:1, size = 10000, 
                         replace = TRUE)
data.frame(outcome = bernoulliDraws) %>%
  ggplot(aes(x = outcome)) +
    geom_bar()
```
]
]

---
## Law of large numbers

The law of large numbers states that as a sample size grows, the sample mean gets closer to the expected value, or population mean

.small[
```{r}
myMeans <- data.frame(sampleSize = 1:10000, myMean = NA)
meanFun <- function(inputSampSize, outcomes) {
  return(mean(outcomes[1:inputSampSize]))
}
myMeans$myMean <- sapply(myMeans$sampleSize, meanFun, bernoulliDraws)
head(myMeans)
tail(myMeans)
```
]
---
## Law of large numbers
```{r}
myMeans %>%
  ggplot(aes(x = sampleSize, y = myMean)) +
  geom_line() +
  labs(x = "Sample size",
       y = "Sample mean",
       title = "Illustration of law of large numbers")
```


---
## Sampling from a Bernoulli distribution in R

- For any value of $0 \leq p \leq 1$ (including .5), we can use the `rbinom()` function

- `rbinom()` has the arguments `n, size, prob`, where `n` is the number of draws from the distribution, and `prob` is the probability of success.

- `size` is the "number of trials (zero or more)" -- for the Bernoulli distribution, we use `size = 1`. Let's leave this for now and come back to it after we've discussed the binomial distribution.

```{r}
set.seed(0) # so results are reproducible 
inputP <- .3
bernoulliDraws.3 <- rbinom(n = 100, size = 1, prob = inputP)
bernoulliDraws.3
```

---
## Sampling from a Bernoulli distribution in R

```{r}
mean(bernoulliDraws.3)
data.frame(outcome = bernoulliDraws.3) %>%
  ggplot(aes(x = outcome)) +
    geom_bar()
```

---
## Effect of changing parameter p

.small[
.pull-left[
```{r out.width = "100%"}
set.seed(0) # so results are reproducible 
inputP <- .3
bernoulliDraws.3 <- rbinom(n = 100, size = 1, prob = inputP)
data.frame(outcome = bernoulliDraws.3) %>%
  ggplot(aes(x = outcome)) +
    geom_bar() +
    labs(title = "100 Bernoulli draws, p = .3")
```
]

.pull-right[
```{r out.width = "100%"}
set.seed(0) # so results are reproducible 
inputP <- .7
bernoulliDraws.7 <- rbinom(n = 100, size = 1, prob = inputP)
data.frame(outcome = bernoulliDraws.7) %>%
  ggplot(aes(x = outcome)) +
    geom_bar() +
    labs(title = "100 Bernoulli draws, p = .7")
```
]
]
---
## Recall: Law of large numbers

The law of large numbers states that as the sample size grows, the sample mean gets closer to the expected value, or population mean

- Sample size is the number of draws from the distribution 

- Sample mean is the mean among those samples 

```{r echo = FALSE}
set.seed(0) # so results are reproducible 
inputP <- .3
bernoulliDraws <- rbinom(n = 5000, size = 1, prob = inputP)
myMeans <- data.frame(sampleSize = 1:5000, myMean = NA)
meanFun <- function(inputSampSize, outcomes) {
  return(mean(outcomes[1:inputSampSize]))
}
myMeans$myMean <- sapply(myMeans$sampleSize, meanFun, bernoulliDraws)
```


```{r echo = FALSE}
myMeans %>%
  ggplot(aes(x = sampleSize, y = myMean)) +
  geom_line() +
  labs(x = "Sample size",
       y = "Sample mean",
       title = "Illustration of law of large numbers with p = .3")
```

---
.small[
```{r eval = FALSE}
set.seed(0) # so results are reproducible 
inputP <- .3
bernoulliDraws <- rbinom(n = 5000, size = 1, prob = inputP)
myMeans <- data.frame(sampleSize = 1:5000, myMean = NA)
meanFun <- function(inputSampSize, outcomes) {
  return(mean(outcomes[1:inputSampSize]))
}
myMeans$myMean <- sapply(myMeans$sampleSize, meanFun, bernoulliDraws)
```

```{r} 
head(myMeans)
tail(myMeans)
```
]

---
```{r}
myMeans %>%
  ggplot(aes(x = sampleSize, y = myMean)) +
  geom_line() +
  labs(x = "Sample size",
       y = "Sample mean",
       title = "Illustration of law of large numbers with p = .3")
```


---
## From Bernoulli to binomial... 

- $Y$ takes value 1 with probability $p$ and value 0 with probability $1-p$

- $P(Y=y)=p^y(1-p)^{1-y}$

  - $P(Y=1)=p^1(1-p)^0=p$ (remember $x^0=1$ for any $x$)
  
  - $P(Y=0)=p^0(1-p)^1=1-p$
  
- For the Bernoulli random variable, we don't really need this formality

- However, we want to extend this to more complex settings

- For example, in a randomly-selected group of 3 high school students, how surprising would it be to get 2 who have smoked e-cigarettes in the past month?

- Could consider three draws from a Bernoulli distribution 

---

## Case Study: E-Cigarettes

- The [CDC reports](https://www.cdc.gov/tobacco/data_statistics/fact_sheets/youth_data/tobacco_use/index.htm) that 19.6% of high school students have smoked e-cigarettes in the past 30 days. We'll round this to 20% for simplicity.

- $P(Y=1)=P(Smoker)=p=0.2$ and $P(Y=0)=0.8$

- Now suppose we randomly select two independent high school students and define a new random variable $X$ representing the number of smokers. X can take the values 0, 1, or 2. 

- Let $Y_1$ be the smoking status of the first student and $Y_2$ be the smoking status of the second student, where $Y_j = 1$ if student $j$ smokes and 0 otherwise.

.pull-left[
Next we'll talk about how to get the *probability distribution* of $X$.

]
.pull-right[
| $Y_1$ | $Y_2$ | $X$ | $P(X)$ |
|:----:|:----:|:----:|:----:|
| 0 | 0 | 0 | |
| 1 | 0 | 1 | |
| 0 | 1 | 1 | |
| 1 | 1 | 2 | |
]

---
## Case Study: E-Cigarettes

- Recall: If events A and B are independent, then $P(A \cap B)=P(A)\times P(B).$

- Let $A_1$ be the event that $Y_1=1$ and let $A_2$ be the event that $Y_2=1$. 

- Since the students are independent, 

$$
\begin{aligned}
P(Y_1=Y_2=1) & = P(A_1 \cap A_2) \\
 & = P(A_1) P(A_2) \\
 & = p \times p  \\
 &= 0.2(0.2)\\
 &=0.04. 
\end{aligned}
$$

---
## Case Study: E-Cigarettes

.pull-left[
Now we can fill in the bottom row of the probability distribution of $X$.

]
.pull-right[
| $Y_1$ | $Y_2$ | $X$ | $P(X)$ |
|:----:|:----:|:----:|:----:|
| 0 | 0 | 0 | |
| 1 | 0 | 1 | |
| 0 | 1 | 1 | |
| 1 | 1 | 2 |  $0.02 \times 0.02 =0.04$ |
]

---
## Case Study: E-Cigarettes

.pull-left[
It's straightforward to fill in the rest of the table in the same way
]
.pull-right[
| $Y_1$ | $Y_2$ | $X$ | $P(X)$ |
|:----:|:----:|:----:|:----:|
| 0 | 0 | 0 | $0.8 \times 0.8 = 0.64$ |
| 1 | 0 | 1 | $0.2 \times 0.8 = 0.16$ |
| 0 | 1 | 1 | $0.8 \times 0.2 = 0.16$ |
| 1 | 1 | 2 | $0.2 \times 0.2 =0.04$ |
]

---

## Case Study: E-Cigarettes

.pull-left[

Recall our table:

| $Y_1$ | $Y_2$ | $X$ | $P(X)$ |
|:----:|:----:|:----:|:----:|
| 0 | 0 | 0 | $0.8 \times 0.8 = 0.64$ |
| 1 | 0 | 1 | $0.2 \times 0.8 = 0.16$ |
| 0 | 1 | 1 | $0.8 \times 0.2 = 0.16$ |
| 1 | 1 | 2 | $0.2 \times 0.2 =0.04$ |
]
.pull-right[
We can clean up the table to get the probability distribution of $X$:

| | | | |
|:--:|:--:|:--:|:--:|
| $X$ | 0 | 1 | 2 |
| $P(X=x)$ | 0.64 | 0.32 | 0.04 |

So if we randomly sample two US high schoolers, the probability that both are recent e-cig smokers is 0.04 (4% chance), the probability only one recently smoked is 0.32 (this can happen two ways -- either only the first smoked or only the second smoked), and the probability neither smoked e-cigs recently is 0.64.
]

---
## Case Study: E-Cigarettes

Now suppose we randomly sample 3 independent high school students

| $Y_1$ | $Y_2$ | $Y_3$ | $X$ | $P(X)$ |
|:----:|:----:|:----:|:----:|:----:|
| 0 | 0 | 0 | 0 |  |
| 1 | 0 | 0 | 1 |  |
| 0 | 1 | 0 | 1 |  |
| 0 | 0 | 1 | 1 |  |
| 1 | 1 | 0 | 2 |  |
| 1 | 0 | 1 | 2 |  |
| 0 | 1 | 1 | 2 |  |
| 1 | 1 | 1 | 3 |  |

---

## Case Study: E-Cigarettes

Because these are independent high school students, we can calculate the probabilities in the same manner as before.


| $Y_1$ | $Y_2$ | $Y_3$ | $X$ | $P(X)$ |
|:----:|:----:|:----:|:----:|:----:|
| 0 | 0 | 0 | 0 | 0.8(0.8)(0.8)=0.512 |
| 1 | 0 | 0 | 1 | 0.2(0.8)(0.8)=0.128 |
| 0 | 1 | 0 | 1 | 0.8(0.2)(0.8)=0.128 |
| 0 | 0 | 1 | 1 | 0.8(0.8)(0.2)=0.128 |
| 1 | 1 | 0 | 2 | 0.2(0.2)(0.8)=0.032 |
| 1 | 0 | 1 | 2 | 0.2(0.8)(0.2)=0.032 |
| 0 | 1 | 1 | 2 | 0.8(0.2)(0.2)=0.032 |
| 1 | 1 | 1 | 3 | 0.2(0.2)(0.2)=0.008 |

The probability that 2 of 3 are recent e-cig smokers is $0.032+0.032+0.032=0.096$ or 9.6%

---

## Case Study: E-Cigarettes

.pull-left[
| $Y_1$ | $Y_2$ | $Y_3$ | $X$ | $P(X)$ |
|:----:|:----:|:----:|:----:|:----:|
| 0 | 0 | 0 | 0 | 0.8(0.8)(0.8)=0.512 |
| 1 | 0 | 0 | 1 | 0.2(0.8)(0.8)=0.128 |
| 0 | 1 | 0 | 1 | 0.8(0.2)(0.8)=0.128 |
| 0 | 0 | 1 | 1 | 0.8(0.8)(0.2)=0.128 |
| 1 | 1 | 0 | 2 | 0.2(0.2)(0.8)=0.032 |
| 1 | 0 | 1 | 2 | 0.2(0.8)(0.2)=0.032 |
| 0 | 1 | 1 | 2 | 0.8(0.2)(0.2)=0.032 |
| 1 | 1 | 1 | 3 | 0.2(0.2)(0.2)=0.008 |
]

.pull-right[
The probability distribution of $X$, the number of recent e-cig smokers out of three high school students, is now 
 
| | | | | |
|:--:|:--:|:--:|:--:|:---:|
| $X$ | 0 | 1 | 2 | 3|
| $P(X=x)$ | 0.512 | 0.384 | 0.096 | 0.008 |
 
 ]

---

## Case Study: E-Cigarettes

- Extending to 4 and more students, we can see why computing the probablities by hand, as we've done, is intractable

- We can use the **binomial distribution** to describe this random variable 

---

## Binomial random variable

- The **binomial distribution** gives us the probability of $X$ "successes" from a sequence of $n$ independent Bernoulli trials (size $n$). This is often denoted binomial(n, p).

- In our example, each student would represent an independent Bernoulli trial (either an e-cig smoker, or not).

  - 1 draw from the binomial distribution is made of 3 independent draws from the Bernoulli distribution 

- This distribution involves three assumptions.

  - There is a fixed number $n$ of Bernoulli trials, each of which results in one of two mutually-exclusive outcomes
  
  - The outcomes of the $n$ trials are independent
  
  - The probability of success $p$ is the same for each trial

---

## Binomial distribution

- The probability mass function for the binomial distribution is given by $P(X=x)=\begin{pmatrix} n \\ x \end{pmatrix}p^x(1-p)^{n-x}$ 

- Compare this with $P(Y=y)=p^y(1-p)^{1-y}$ for the Bernoulli distribution.

- First, look at the second part, $p^x(1-p)^{n-x}$. This is just multiplying the right combination of $p$ and $1-p$ as in the previous tables.

  - There will be a total of $n$ terms being multiplied, one probability for each draw of the distribution (each student in this case)

  - For example, if we want the probability of 3 e-cig smokers, $X=3$, the second part is $p^x(1-p)^{n-x}=0.2^3(0.8)^0=0.008$, just as in the table.

---

## Binomial distribution

$$P(X=x)=\begin{pmatrix} n \\ x \end{pmatrix}p^x(1-p)^{n-x}$$

- If we want the probability of 2 e-cig smokers and 1 non-smoker, i.e., $x=2$, the second part is $p^x(1-p)^{n-x}=0.2^2(0.8)^{3-2}=0.032$, which is what we see in any single row in which we have two smokers and one non-smoker.

  - This is the probability of any one specific combination of 2 smokers and 1 nonsmoker. Then we need to figure out how many combinations of 2 smokers and 1 nonsmoker we could get.

- The first part, $\begin{pmatrix}n \\x \end{pmatrix}$, accounts for all the possible ways in which we can have 2 smokers out of 3 people.

---

## Combinations

- The first part, $\begin{pmatrix} n \\ x \end{pmatrix}$, is pronounced "n choose x" 

- This is called a **combination**.

- In this particular setting, it is also called the *binomial coefficient*. 

- It is a formula for the number of ways to pick $x$ subjects from a larger group of $n$ and is defined as $$\begin{pmatrix} n \\ x \end{pmatrix} = \frac{n!}{x!(n-x)!}.$$

- n! (pronounced "n factorial") is shorthand for the recursive multiplication $n!=n(n-1)(n-2)\cdots (1)$. 

  - $3!=3(2)(1)=6$
  - $4!=4(3)(2)(1)=24$, and so forth. 
  - We define $0!=1$.

---

## Combinations
- In R, combinations are done using `choose(n, k)`

- Factorials are `factorial(x)`, e.g., 
```{r}
factorial(3)
```

- How many ways can we pick 3 subjects from a group of 3? Using the binomial coefficient, we see it is $\begin{pmatrix}3 \\ 3 \end{pmatrix}=\frac{3!}{3!0!}=\frac{3(2)(1)}{3(2)(1)1}=1$. In R,
```{r}
choose(3, 3)
```

---

## Combinations

Going back to the table, we see there are 3 ways to pick 2 subjects from a group of 3 students.  

.small[

| $Y_1$ | $Y_2$ | $Y_3$ | $X$ | $P(X)$ |
|:----:|:----:|:----:|:----:|:----:|
| 0 | 0 | 0 | 0 | 0.8(0.8)(0.8)=0.512 |
| 1 | 0 | 0 | 1 | 0.2(0.8)(0.8)=0.128 |
| 0 | 1 | 0 | 1 | 0.8(0.2)(0.8)=0.128 |
| 0 | 0 | 1 | 1 | 0.8(0.8)(0.2)=0.128 |
| 1 | 1 | 0 | 2 | 0.2(0.2)(0.8)=0.032 |
| 1 | 0 | 1 | 2 | 0.2(0.8)(0.2)=0.032 |
| 0 | 1 | 1 | 2 | 0.8(0.2)(0.2)=0.032 |
| 1 | 1 | 1 | 3 | 0.2(0.2)(0.2)=0.008 |

]

We can also calculate $\begin{pmatrix}3 \\ 2 \end{pmatrix}=\frac{3!}{2!1!}=\frac{3(2)(1)}{(2)(1)(1)}=3$.

---

## Binomial probabilities

Putting the ingredients together, we have 

$$
\begin{aligned}
P(X = 2) &=  \begin{pmatrix} n \\ x \end{pmatrix}p^x(1-p)^{n-x}\\
&= \begin{pmatrix} 3 \\ 2 \end{pmatrix}p^2(1-p)^{3-2}\\
&=3(0.2)^2(0.8)^{3-2}\\
&=3(.032) \\
&=.096
\end{aligned}
$$
This is the same as we saw in the table. 

---

## Binomial probabilities 

- We saw how to calculate $P(X = 2)$. How about $P(X = 3)$?

$$
\begin{aligned}
P(X = x) &=\begin{pmatrix} n \\ x \end{pmatrix} p^x(1-p)^{n-x} \\
&= \frac{n!}{x!(n-x)!}p^x(1-p)^{n-x} 
\end{aligned}
$$

- Using the binomial distribution, the probability of getting 3 recent e-cig smokers in our sample of 3 students is

$$\begin{aligned}
P(X = 3)& = \begin{pmatrix} 3 \\ 3 \end{pmatrix}0.2^3(1-0.2)^{3-3}  \\
& = \frac{3!}{3!(3-3)!}0.2^3(1-0.2)^{3-3} \\
 & = \frac{3(2)(1)}{3(2)(1)1}0.2^3(0.8)^{0}=0.2^3 \\
 & = 0.008.
 \end{aligned}$$

- In this way, we can derive the probability distribution

---
## Binomial probabilites in R

- Luckily, we don't have to do this by hand

- In R, we can use `dbinom()`

- From the help page: `dbinom()` has the arguments `x, size, prob, log = FALSE`
  - `x` are the values of $x$ that we want probabilities for; here it is 0, 1, 2 and 3
  - `size` is the number of Bernoulli trials; here it is 3
  - `prob` is the probability of success; here it is .2

.pull-left[
```{r}
dbinom(x = 0:3, size = 3, 
       prob = .2)
```
]

.pull-right[
The probability distribution of $X$, the number of recent e-cig smokers out of three high school students, is now 
 
| | | | | |
|:--:|:--:|:--:|:--:|:---:|
| $X$ | 0 | 1 | 2 | 3|
| $P(X=x)$ | 0.512 | 0.384 | 0.096 | 0.008 |
 
 ]


---
## Bernoulli vs. binomial random variable

- Recall: The **binomial distribution** gives us the probability of $X$ "successes" from a sequence of $n$ independent Bernoulli trials. 

- In our example, each student would be an independent Bernoulli trial (either an e-cig smoker, or not).

  - Bernoulli random variable: whether or not a student is a recent e-cigarette smoker 

  - Binomial random variable: number of students, out of 3, who are recent e-cigarette smokers

  - One observation of this binomial random variable is composed of 3 Bernoulli trials ("size 3")
  
- The Bernoulli distribution can be thought of as a special case of the binomial distribution, with 1 "trial"

---
## Mean and variance of binomial random variable

For a binomial random variable with number of trials $n$ and probability of success $p$, $E(X) = np$ and $Var(X) = n(p)(1-p)$

- In our example, $p = .2$, $n = 3$, so $E(X) = .6$ and $Var(X) = 3*.2*.8 = .48$

--

- The Bernoulli distribution is a special case of the binomial distribution, with $n = 1$. Recall: $E(X) = p$ and $Var(X) = p(1-p)$.



---
## Summary
- Common probability distributions

  - Bernoulli 
  
  - Binomial 



