<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Normal Distribution, Sampling distributions</title>
    <meta charset="utf-8" />
    <meta name="author" content="Xiao Hui Tai" />
    <meta name="date" content="2022-11-07" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/default-fonts.css" rel="stylesheet" />
    <link href="libs/font-awesome/css/all.css" rel="stylesheet" />
    <link href="libs/font-awesome/css/v4-shims.css" rel="stylesheet" />
    <link href="libs/panelset/panelset.css" rel="stylesheet" />
    <script src="libs/panelset/panelset.js"></script>
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Normal Distribution, Sampling distributions
]
.subtitle[
## <br><br> STA35A: Statistical Data Science 1
]
.author[
### Xiao Hui Tai
]
.date[
### November 7, 2022
]

---





layout: true
  
&lt;!-- &lt;div class="my-footer"&gt; --&gt;
&lt;!-- &lt;span&gt; --&gt;
&lt;!-- &lt;a href="https://datasciencebox.org" target="_blank"&gt;datasciencebox.org&lt;/a&gt; --&gt;
&lt;!-- &lt;/span&gt; --&gt;
&lt;!-- &lt;/div&gt;  --&gt;

---

&lt;style type="text/css"&gt;
.tiny .remark-code { font-size: 70%; }
.small .remark-code { font-size: 80%; }
.tiny { font-size: 60%; }
.small { font-size: 80%; }
&lt;/style&gt;




## Reminders/announcements 

- Veterans Day this Friday, no class! 

- No homework assigned this Friday; HW 6 will be released 11/14 (after Veterans Day), due 11/21 

- Next week's schedule: 

  - Monday lecture, HW 6 assigned
  - Wednesday review (Jed Harwood)
  - Thursday OH (XHT, 12:10-2pm MSB 4242)
  - Friday midterm 2

---
## Midterm 2 on 11/18

- Will cover material from 10/24 (Lecture 13), until next Monday's lecture (Lecture 21)

- Closed-book. These formulas will be provided: 
  - **Bayes' theorem**: `\(P(A \mid B) =\frac{P(B \mid A)P(A)}{P(B)}\)`.
  - **Probability mass functions**:  
      - Binomial: `\(P(X=x)=\begin{pmatrix} n \\ x \end{pmatrix}p^x(1-p)^{n-x}\)`  
      - Poisson: `\(P(X = x) = \frac{\lambda^x e^{-\lambda}}{x!}\)`, `\(\lambda &gt; 0\)`
  - **Probability density function for normal distribution**:
  `\(f(x)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{1}{2}\frac{(x-\mu)^2}{\sigma^2}}\)`

- You don't need your computers or calculators 

- There will be no make-up exams

- Drop policy for exams: 1 midterm may be dropped or used with 50% penalty on the final

---
## Recap

- Common probability distributions: Normal

  - Theoretical properties: probability density function, parameters, mean and variance, effect of varying parameters
  
  - R functions:
  
      - `dnorm()` for densities 
      - `pnorm()` for `\(P(X\leq x)\)`
      - `rnorm()` for random sample
    
  - Standard normal distribution


---
## Today

- Common probability distributions: Normal

  - More about the standard normal distribution

  - R functions:
      - `qnorm()`
    
  - Sum of independent normal distributions 
  
- Sampling distributions
  
---
## More about the standard normal distribution 

- We saw earlier that `\(P(Z \leq 0) = .5\)`. This is because the standard normal distribution is symmetric with mean 0.


```r
pnorm(0) # default value of mean = 0 and sd = 1
```

```
## [1] 0.5
```

- Tail probabilities of the standard normal distribution 

  - The symmetry of the normal distribution allows us to calculate the probability of values falling in the tails
  
  - For any `\(z\)`-score, `\(P(Z \leq -z) = P(Z \geq z)\)`

&lt;img src="img/stdnorm5.png" width="20%" style="display: block; margin: auto;" /&gt;

---
## Quantiles for the normal distribution 

- Quantiles are cut points dividing the range of a probability distribution into continuous intervals

- Recall: quartiles (four groups) and percentiles (100 groups)

- `\(P(X \leq q) = p\)`, where `\(q\)` is the quantile (think of value on the horizontal axis), e.g., `\(P(Z \leq 0) = .5\)`

- Recall: `pnorm(q, mean, sd)` for `\(P(X\leq x)\)`, or `\(P(Z \leq z)\)` for standard normal. `pnorm()` returns the probability, `p`


```r
pnorm(q = 0, mean = 0, sd = 1)
```

```
## [1] 0.5
```

- `qnorm(p, mean, sd)` for the quantile, e.g., `\(P(X \leq \ ?) = p\)`. `qnorm()` returns the quantile, `q`
  

```r
qnorm(p = .5, mean = 0, sd = 1)
```

```
## [1] 0
```

---
## Important reference points for the normal distribution 

- For the standard normal, z-scores (quantiles in R) corresponding to particular probabilities are often written as `\(z_p\)`, where `\(p\)` denotes the probability, e.g., `\(z_{.5} = 0\)`

- The z-scores corresponding to probabilities of 0.025 (2.5%) in the left and right tails are important reference points. Specifically, `\(z_{.025} \approx -1.96\)`

- In R, `qnorm(.025)` returns the z-score corresponding to .025, `\(z_{.025}\)`, i.e., 2.5% probability in the left tail, so we should get -1.96. By symmetry, `qnorm(.975)` will return 1.96.

.pull-left[

```r
qnorm(.025)
```

```
## [1] -1.959964
```

```r
qnorm(.975)
```

```
## [1] 1.959964
```
]

.pull-right[
&lt;img src="img/stdnorm5.png" width="70%" style="display: block; margin: auto;" /&gt;
]
---
## Important reference points for the normal distribution 


.pull-left[

```r
pnorm(1.96)
```

```
## [1] 0.9750021
```

```r
pnorm(1.96, lower.tail = FALSE)
```

```
## [1] 0.0249979
```

&lt;img src="img/stdnorm1.png" width="78%" style="display: block; margin: auto;" /&gt;
]
.pull-right[

```r
pnorm(-1.96)
```

```
## [1] 0.0249979
```

```r
pnorm(-1.96, lower.tail = FALSE)
```

```
## [1] 0.9750021
```

&lt;img src="img/stdnorm2.png" width="70%" style="display: block; margin: auto;" /&gt;
]

---
## Standard normal table

- A **standard normal table** allows us to calculate values based on the standard normal distribution.

- It tells us how much area is under the normal curve to the *left* of the specified value (lower tail area). Sometimes the table shows the complement of this probability (upper or *right* tail area).

&lt;img src="img/cumprob.png" width="50%" style="display: block; margin: auto;" /&gt;

- With modern computing, we don't need to rely on these tables to get the desired probabilities, but you often find them in the back of statistics textbooks.

---
## Standard normal table

&lt;img src="img/normaltable.png" width="60%" style="display: block; margin: auto;" /&gt;

---
## Standard normal table

.pull-left[
What is the probability of a shoe size bigger than 13 (z-score 1.33)?

&lt;img src="img/normalcurveupper.png" width="100%" style="display: block; margin: auto;" /&gt;
]
--
.pull-right[
.small[

```r
pnorm(13, mean = 11, sd = 1.5, lower.tail = FALSE)
```

```
## [1] 0.09121122
```

```r
pnorm(2/1.5, lower.tail = FALSE)
```

```
## [1] 0.09121122
```

```r
1 - pnorm(2/1.5)
```

```
## [1] 0.09121122
```
]
]

---
## Sum of independent normal random variables 

- Important property: Any linear combination of normal random variables is a normal random variable with expectation and variance given by the formulas for expected value and variance of linear combinations (Lecture 15)

- Recall: A linear combination of two random variables, `\(X\)` and `\(Y\)`, is of the form `\(aX+bY\)`, where `\(a\)`
and `\(b\)` are constants

- Recall: 
  - `\(E(aX + bY) = aE(X) + bE(Y)\)`
  - For a linear combination of **independent** random variables `\(Var(aX + bY) = a^2 Var(X) + b^2 Var(Y)\)`

- Hence if `\(X \sim N(\mu_x, \sigma_x^2)\)` and `\(Y \sim N(\mu_y, \sigma_y^2)\)` are independent, `\(W = X + Y \sim N(\mu_x + \mu_y, \sigma_x^2 + \sigma_y^2)\)`

- Extends to more than two random variables in the linear combination. Note also that `\(b\)` can be negative, e.g., `\(E(X - Y) = E(X) - E(Y)\)` and `\(Var(X - Y) = Var(X) + Var(Y)\)`.

---
## Summary: Distributions in R

- For each distribution, R has a family of commands, starting with the letters `d`, `p`, `q` and `r`
  - `d` for density
  - `p` for cumulative density up to input value `\(P(X \leq x)\)`. Think of `\(P(X \leq q) = p\)`
  - `q` for the quantile, e.g., `\(P(X \leq \ ?) = p\)`
  - `r` for a random sample from the distribution

---
## Course content 

1. Fundamentals of R
  - Overview of data types and structures
  - Data manipulation and data visualization tools  

2. Descriptive statistics for numerical and categorical data

3. Probability
  - Rules of probability computation; conditional probability
  - Basic probability models: Binomial, Normal and Poisson 

4. **Statistical inference**
  - **Sampling distributions of sample mean and sample proportion**
  - Hypothesis testing and confidence intervals for population mean and population proportion

---

## Recall: What is statistical inference?
- **Descriptive statistics** are numbers that are used to summarize and describe data. Descriptive statistics do not necessarily generalize beyond the data, because of sampling variability

- **Inferential statistics** or **statistical inference** gives us an idea about how sample means (for example) from different samples are likely to vary from each other and from the population mean 

- **Statistical inference** allows us to draw conclusions about the larger population. It is the process of using sample data to make conclusions about the underlying population the sample came from.

- If the sample is **representative**, then we can use the tools of probability and statistical inference to make generalizable conclusions to the broader population of interest.

.pull-left[
&lt;img src="img/soup.png" width="50%" style="display: block; margin: auto;" /&gt;
]

.pull-right[
Similar to tasting a spoonful of soup while cooking to make an inference about the entire pot.
]

---
## Recall: Example of statistical inference in action 

When a sample statistic is used to estimate a population parameter, it will be accompanied by a margin of error

&lt;img src="img/approval.png" width="85%" style="display: block; margin: auto;" /&gt;

.tiny[
Source: https://www.rasmussenreports.com/public_content/politics/biden_administration/prez_track_sep23
]

---
## Recall: Many Topics in Statistical Inference

- Fundamentals: probability, distributions, random variables, ...

- **Sampling**

- Hypothesis testing

- Point estimates and confidence intervals

- Modeling: Linear regression, analysis of variance, nonparametric models, machine learning, ... 

---
## Sampling Distribution of the Sample Mean 

Recall our shoe size example, where wearers of men's shoe sizes follow a `\(N(11, 1.5^2)\)` distribution.

Say we are interested in the sample mean of shoe sizes. We have a sample of 1000 observations.


```r
set.seed(0)
sampled1000_1 &lt;- rnorm(1000, 11, 1.5)
head(sampled1000_1, 20)
```

```
##  [1] 12.894431 10.510650 12.994699 12.908644 11.621962  8.690075
##  [7]  9.607149 10.557919 10.991349 14.606980 12.145390  9.801486
## [13]  9.278514 10.565808 10.551177 10.382734 11.378335  9.662118
## [19] 11.653525  9.143692
```

```r
mean(sampled1000_1) 
```

```
## [1] 10.97626
```

---
## Sampling Distribution of the Sample Mean 
Now we repeat the experiment, i.e., get a different sample of 1000 observations. 


```r
set.seed(10)
sampled1000_2 &lt;- rnorm(1000, 11, 1.5)
head(sampled1000_2, 20)
```

```
##  [1] 11.028119 10.723621  8.943004 10.101248 11.441818 11.584691
##  [7]  9.187886 10.454486  8.559991 10.615282 12.652669 12.133672
## [13] 10.642650 12.481167 12.112085 11.134021  9.567584 10.707274
## [19] 12.388282 11.724468
```

```r
mean(sampled1000_2) 
```

```
## [1] 11.01706
```

```r
all.equal(mean(sampled1000_1), mean(sampled1000_2))
```

```
## [1] "Mean relative difference: 0.003717704"
```
---
## Sampling Distribution of the Sample Mean 

If we repeat the experiment an infinite number of times, what distribution of sample means would we get? This is known as the **sampling distribution**.

1. Take a sample of size `\(n\)` and calculate its mean `\(\overline{x}_1\)`
2. Take a second sample of the same size and calculate its mean `\(\overline{x}_2\)`
3. Repeat this many times to get a dataset of sample means `\(\overline{x}_1, \overline{x}_2, \ldots\)`

What is the distribution of the *statistics* `\(\overline{x}_1, \overline{x}_2, \overline{x}_3, \ldots\)`?

The sample mean `\(\overline{X}\)`, is defined as `\(\overline{X} = \frac{\sum_{i = 1}^n X_i}{n}\)`. Each realization above is a draw from `\(\overline{X}\)`, denoted with a small letter `\(\overline{x}\)`.

Here we consider `\(X_1, ..., X_n\)` that are **independent and identically distributed**. (E.g., `\(X_1, ... X_n \sim N(11, 1.5^2)\)` for the shoe size distribution.)

---
## Sampling Distribution of the Sample Mean 

1. Take a sample of size `\(n\)` and calculate its mean `\(\overline{x}_1\)`
2. Take a second sample of the same size and calculate its mean `\(\overline{x}_2\)`
3. Repeat this many times to get a dataset of sample means `\(\overline{x}_1, \overline{x}_2, \ldots\)`

We cannot repeat an infinite number of times, but we do this 10,000 times in R.

.small[

```r
set.seed(0)
repeat10000 &lt;- t(replicate(n = 10000, rnorm(1000, 11, 1.5)))
str(repeat10000)
```

```
##  num [1:10000, 1:1000] 12.9 10.6 11.7 10.2 12.2 ...
```

```r
head(rowMeans(repeat10000), 20)
```

```
##  [1] 10.97626 10.96282 11.10221 11.00373 11.00616 11.02959
##  [7] 10.99695 11.06309 10.92670 11.08920 10.97525 10.95720
## [13] 10.95437 11.07828 11.02202 11.05240 11.00455 10.95824
## [19] 10.96431 10.96413
```

```r
means10000 &lt;- rowMeans(repeat10000)
```
]

---
## Sampling Distribution of the Sample Mean 



```r
data.frame(shoesMean = means10000) %&gt;%
  ggplot(aes(x = shoesMean)) +
  geom_density() +
  labs(x = "Mean of sample of size 1000",
       y = "Density",
       title = "Sampling distribution from N(11, 1.5^2)")
```

&lt;img src="lecture19_files/figure-html/unnamed-chunk-22-1.png" width="60%" style="display: block; margin: auto;" /&gt;

---
## Sampling Distribution of the Sample Mean 

.pull-left[
&lt;img src="lecture19_files/figure-html/unnamed-chunk-23-1.png" width="100%" style="display: block; margin: auto;" /&gt;
]
.pull-right[
How would we describe this distribution?

- Center

- Spread

- Shape
]
---
## Sampling Distribution of the Sample Mean 

.pull-left[
&lt;img src="lecture19_files/figure-html/unnamed-chunk-24-1.png" width="100%" style="display: block; margin: auto;" /&gt;
]
.small[
.pull-right[
How would we describe this distribution?

- Center
  - The distribution is centered at 11, which is the same as the population parameter

- Spread
  - The variability looks to be much smaller than the original distribution (the original distribution has standard deviation 1.5)

- Shape
  - The distribution is symmetric and bell-shaped, and it resembles a normal distribution.
]
]

---
## Effect of changing sample size

- Earlier we used a sample size of 1000. What if we used a sample size of 50? 


```r
set.seed(0)
repeat10000_n50 &lt;- t(replicate(n = 10000, rnorm(50, 11, 1.5)))
str(repeat10000_n50)
```

```
##  num [1:10000, 1:50] 12.89 11.4 12.17 13.21 9.43 ...
```

```r
head(rowMeans(repeat10000_n50), 20)
```

```
##  [1] 11.03590 11.03211 10.68366 11.17968 11.06924 11.13242
##  [7] 11.03710 10.97258 10.96971 10.88460 10.93739 10.80427
## [13] 11.03709 10.90858 10.84133 11.32991 11.04654 10.74770
## [19] 10.98847 10.88685
```

```r
means10000_n50 &lt;- rowMeans(repeat10000_n50)
```

---
## Effect of changing sample size

.tiny[
.pull-left[

```r
data.frame(shoesMean = means10000, sampleSize = 1000) %&gt;%
  bind_rows(
    data.frame(means10000_n50, sampleSize = 50) %&gt;%
      rename(shoesMean = means10000_n50)
  )  %&gt;%
  ggplot(aes(x = shoesMean,
             fill = as.factor(sampleSize))) +
  geom_density() +
  labs(x = "Mean shoe size",
       y = "Density",
       title = "Sampling distribution from N(11, 1.5^2)",
       fill = "Sample size")  +
  scale_fill_viridis_d()
 # guides(fill = "none")
```
]
]

.pull-right[
&lt;img src="lecture19_files/figure-html/unnamed-chunk-27-1.png" width="100%" style="display: block; margin: auto;" /&gt;
]

- What do you notice about the spread? 

--
  - A larger sample size produces more precise estimates 
  - We will formalize this intuition using the Central Limit Theorem 
  
---
## Note on sampling distributions

- From Open Intro Statistics: "Sampling distributions are never observed, but we keep them in mind."

- In real-world applications, we never actually observe the sampling distribution

- Even in simulations, we cannot run experiments an infinite number of times to generate the sampling distribution 

- Yet it is useful to think of a sample statistic as coming from such a hypothetical distribution

- Understanding the sampling distribution will help us characterize and make sense of the sample statistics that we do observe.


---
## Sampling distributions, confidence intervals and hypothesis testing

Knowing the sampling distribution can help us

- Estimate a population parameter as point estimate `\(\pm\)` margin of error, where the margin of error is comprised of a measure of how confident we want to be and the sample statistic's variability. (Coming soon: confidence intervals)

- Test whether a population parameter is equal to some value, by evaluating how likely it is that we have obtained the observed sample statistic, if the population parameter is indeed that value. (Coming soon: hypothesis testing)

---
## Summary

- Common probability distributions: Normal

  - More about the standard normal distribution

  - R functions:
      - `qnorm()` for the value corresponding to an input probability, e.g., `\(P(X \leq \ ?) = p\)`
    
  - Sum of independent normal distributions: any linear combination of normal random variables is a normal random variable
  
- Sampling distributions

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
